<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Lecture9 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="偏差-方差测试误差分解为”bias”和”variance”项, 并研究它们中的每一个是如何受到模型参数化选择及其权衡的影响的。使用偏差-方差权衡, 我们将讨论何时会发生过拟合和欠拟合并避免。 测试误差和训练误差之间的差异通常被称为泛化差距。 通常, 在偏差和方差之间存在权衡。如果我们的模型太“简单”并且参数很少, 那么它可能会有很大的偏差（但方差很小）, 并且它通常会遭受欠拟合。如果它太“复杂”并">
<meta property="og:type" content="article">
<meta property="og:title" content="Lecture9">
<meta property="og:url" content="http://example.com/2025/02/13/Lecture9/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="偏差-方差测试误差分解为”bias”和”variance”项, 并研究它们中的每一个是如何受到模型参数化选择及其权衡的影响的。使用偏差-方差权衡, 我们将讨论何时会发生过拟合和欠拟合并避免。 测试误差和训练误差之间的差异通常被称为泛化差距。 通常, 在偏差和方差之间存在权衡。如果我们的模型太“简单”并且参数很少, 那么它可能会有很大的偏差（但方差很小）, 并且它通常会遭受欠拟合。如果它太“复杂”并">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2025/02/13/Lecture9/image.png">
<meta property="og:image" content="http://example.com/2025/02/13/Lecture9/image9_2.png">
<meta property="og:image" content="http://example.com/2025/02/13/Lecture9/image9_3.png">
<meta property="article:published_time" content="2025-02-13T11:34:46.000Z">
<meta property="article:modified_time" content="2025-02-13T11:57:53.483Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="2018Autumn">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/02/13/Lecture9/image.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Lecture9" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/02/13/Lecture9/" class="article-date">
  <time class="dt-published" datetime="2025-02-13T11:34:46.000Z" itemprop="datePublished">2025-02-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Lecture9
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="偏差-方差"><a href="#偏差-方差" class="headerlink" title="偏差-方差"></a>偏差-方差</h1><p>测试误差分解为”bias”和”variance”项, 并研究它们中的每一个是如何受到模型参数化选择及其权衡的影响的。使用偏差-方差权衡, 我们将讨论何时会发生过拟合和欠拟合并避免。</p>
<p>测试误差和训练误差之间的差异通常被称为泛化差距。</p>
<p>通常, 在偏差和方差之间存在权衡。如果我们的模型太“简单”并且参数很少, 那么它可能会有很大的偏差（但方差很小）, 并且它通常会遭受欠拟合。如果它太“复杂”并且有很多参数, 那么它可能会受到大方差的影响（但偏差较小）, 从而导致过拟合。偏差和方差之间的典型权衡如下<br><img src="/2025/02/13/Lecture9/image.png" alt="alt text"><br><strong>图8.8 偏差-方差权衡曲线</strong><br>偏差-方差权衡曲线或测试误差曲线并不普遍地遵循图8.8的形状, 至少在简单地通过参数数量来衡量模型复杂性时不是普遍的。</p>
<h2 id="二次下降现象"><a href="#二次下降现象" class="headerlink" title="二次下降现象"></a>二次下降现象</h2><h3 id="模型双下降"><a href="#模型双下降" class="headerlink" title="模型双下降"></a>模型双下降</h3><ul>
<li><strong>传统观点</strong>：随着模型复杂度的增加, 测试误差先减小（欠拟合区域）, 然后增大（过拟合区域）, 形成一个U形曲线。</li>
<li><strong>双下降现象</strong>：在模型复杂度非常高时, 测试误差会再次下降, 形成第二个下降区域, 称为“双下降”。</li>
</ul>
<p><img src="/2025/02/13/Lecture9/image9_2.png" alt="alt text"></p>
<h3 id="逐样本双下降"><a href="#逐样本双下降" class="headerlink" title="逐样本双下降"></a>逐样本双下降</h3><p>随着样本量的增加, 测试误差并不是单调递减的。相反, 如图9_3所示, 测试误差先减小, 然后增大, 并在示例数量（由n表示）与参数数量（由d表示）相似时达到峰值, 然后再次减小。我们称之为样本双下降现象。<br>当n≈d时, 还有其他算法可以实现较小的测试误差, 但这些实验中评估的算法未能做到这一点。<br>事实上, 可以通过优化正则化, n≈d区域的测试误差可以得到显著改善, 模型和样本双下降都得到了缓解<br><img src="/2025/02/13/Lecture9/image9_3.png" alt="alt text"></p>
<h2 id="样本复杂性界限"><a href="#样本复杂性界限" class="headerlink" title="样本复杂性界限"></a>样本复杂性界限</h2><h3 id="切尔诺夫界（Chernoff-Bound）-霍夫丁不等式"><a href="#切尔诺夫界（Chernoff-Bound）-霍夫丁不等式" class="headerlink" title="切尔诺夫界（Chernoff Bound）&#x2F;霍夫丁不等式"></a>切尔诺夫界（Chernoff Bound）&#x2F;霍夫丁不等式</h3><p>切尔诺夫界是概率论中的一个重要不等式，用于描述独立同分布（i.i.d.）随机变量的均值与其期望值之间偏差的概率上界。以下是其定义和应用：</p>
<hr>
<h4 id="1-问题设定"><a href="#1-问题设定" class="headerlink" title="1. 问题设定"></a><strong>1. 问题设定</strong></h4><ul>
<li>设 ( Z_1, Z_2, \dots, Z_n ) 是 ( n ) 个独立同分布的随机变量，服从伯努利分布 ( \text{Bernoulli}(\phi) )。<ul>
<li>( P(Z_i &#x3D; 1) &#x3D; \phi )</li>
<li>( P(Z_i &#x3D; 0) &#x3D; 1 - \phi )</li>
</ul>
</li>
<li>定义 ( \hat{\phi} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n Z_i ) 为这些随机变量的均值。</li>
<li>设 ( \gamma &gt; 0 ) 为任意固定常数。</li>
</ul>
<hr>
<h4 id="2-切尔诺夫界（Chernoff-Bound）-霍夫丁不等式"><a href="#2-切尔诺夫界（Chernoff-Bound）-霍夫丁不等式" class="headerlink" title="2. 切尔诺夫界（Chernoff Bound）&#x2F;霍夫丁不等式"></a><strong>2. 切尔诺夫界（Chernoff Bound）&#x2F;霍夫丁不等式</strong></h4><p>切尔诺夫界给出了 ( \hat{\phi} ) 与真实值 ( \phi ) 之间偏差的概率上界：<br>$$<br>P(|\phi - \hat{\phi}| &gt; \gamma) \leq 2 \exp(-2\gamma^2 n)<br>$$</p>
<hr>
<h4 id="3-训练误差（Training-Error）"><a href="#3-训练误差（Training-Error）" class="headerlink" title="3. 训练误差（Training Error）"></a><strong>3. 训练误差（Training Error）</strong></h4><p>训练误差（也称为经验风险或经验误差）是模型在训练集上的错误率：<br>$$<br>\hat{\epsilon}(h) &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n \mathbb{1}{h(x^{(i)}) \neq y^{(i)}}<br>$$<br>其中：</p>
<ul>
<li>( \mathbb{1}{\cdot} ) 是指示函数，当条件成立时取值为 1，否则为 0。</li>
<li>( n ) 是训练样本的数量。</li>
</ul>
<hr>
<h4 id="4-泛化误差（Generalization-Error）"><a href="#4-泛化误差（Generalization-Error）" class="headerlink" title="4. 泛化误差（Generalization Error）"></a><strong>4. 泛化误差（Generalization Error）</strong></h4><p>泛化误差是模型在分布 ( D ) 上的期望错误率，反映了模型在未见数据上的表现：<br>$$<br>\epsilon(h) &#x3D; P_{(x,y) \sim D}(h(x) \neq y)<br>$$<br>其中：</p>
<ul>
<li>( (x, y) ) 是从分布 ( D ) 中抽取的新样本。</li>
</ul>
<hr>
<h4 id="4-PAC-假设"><a href="#4-PAC-假设" class="headerlink" title="4. PAC 假设"></a><strong>4. PAC 假设</strong></h4><ul>
<li>假设训练数据和测试数据来自相同的分布 ( D )，这是 PAC（Probably Approximately Correct）学习理论中的一个基本假设。</li>
<li>这一假设确保了训练误差能够有效反映泛化误差。</li>
</ul>
<hr>
<h3 id="偏差-方差分解（Bias-Variance-Decomposition）"><a href="#偏差-方差分解（Bias-Variance-Decomposition）" class="headerlink" title="偏差-方差分解（Bias-Variance Decomposition）"></a>偏差-方差分解（Bias-Variance Decomposition）</h3><p>在机器学习中, 偏差-方差分解是一种用于分析模型预测误差的方法。它将误差分解为三个部分：不可避免的噪声、偏差和方差。</p>
<hr>
<h4 id="1-误差分解公式"><a href="#1-误差分解公式" class="headerlink" title="1. 误差分解公式"></a><strong>1. 误差分解公式</strong></h4><p>对于给定的输入 ( x ), 均方误差（MSE）可以分解为：<br>$$<br>\text{MSE}(x) &#x3D; \sigma^2 + \text{Bias}^2 + \text{Variance}<br>$$<br>其中：</p>
<ul>
<li>( \sigma^2 )：<strong>不可避免的噪声</strong>, 表示数据中的随机噪声, 无法通过模型预测。</li>
<li>( \text{Bias}^2 )：<strong>偏差的平方</strong>, 表示模型预测值与真实值之间的系统性误差。</li>
<li>( \text{Variance} )：<strong>方差</strong>, 表示模型对训练数据的随机性敏感程度。</li>
</ul>
<hr>
<h4 id="2-偏差（Bias）"><a href="#2-偏差（Bias）" class="headerlink" title="2. 偏差（Bias）"></a><strong>2. 偏差（Bias）</strong></h4><ul>
<li><strong>定义</strong>：偏差是模型预测值的期望与真实值之间的差异：<br>$$<br>\text{Bias} &#x3D; h^*(x) - h_{\text{avg}}(x)<br>$$<br>其中：<ul>
<li>( h^*(x) ) 是真实值。</li>
<li>( h_{\text{avg}}(x) ) 是模型在无限数据集上的平均预测值。</li>
</ul>
</li>
<li><strong>含义</strong>：偏差反映了模型由于表达能力不足而引入的系统性误差。即使有无限数据, 偏差仍然存在。</li>
</ul>
<hr>
<h4 id="3-方差（Variance）"><a href="#3-方差（Variance）" class="headerlink" title="3. 方差（Variance）"></a><strong>3. 方差（Variance）</strong></h4><ul>
<li><strong>定义</strong>：方差是模型预测值在不同数据集上的波动：<br>$$<br>\text{Variance} &#x3D; \mathbb{E}[(h_{\text{avg}}(x) - h_S(x))^2]<br>$$<br>其中：<ul>
<li>( h_S(x) ) 是模型在特定数据集 ( S ) 上的预测值。</li>
</ul>
</li>
<li><strong>含义</strong>：方差反映了模型对训练数据随机性的敏感程度。随着数据集规模的增加, 方差通常会减小。</li>
</ul>
<hr>
<h4 id="4-噪声（Noise）"><a href="#4-噪声（Noise）" class="headerlink" title="4. 噪声（Noise）"></a><strong>4. 噪声（Noise）</strong></h4><ul>
<li><strong>定义</strong>：噪声是数据中无法预测的随机误差：<br>$$<br>\sigma^2 &#x3D; \text{Var}(\xi)<br>$$<br>其中 ( \xi ) 是噪声项。</li>
<li><strong>含义</strong>：噪声是不可避免的, 因为它与模型的表达能力无关。</li>
</ul>
<hr>
<h4 id="5-偏差-方差的权衡"><a href="#5-偏差-方差的权衡" class="headerlink" title="5. 偏差-方差的权衡"></a><strong>5. 偏差-方差的权衡</strong></h4><ul>
<li><strong>高偏差</strong>：模型过于简单, 无法捕捉数据的真实分布, 导致欠拟合（underfitting）。</li>
<li><strong>高方差</strong>：模型过于复杂, 对训练数据的随机性过于敏感, 导致过拟合（overfitting）。</li>
<li><strong>目标</strong>：通过调整模型复杂度, 找到偏差和方差之间的最佳平衡点。</li>
</ul>
<hr>
<h4 id="6-分类问题中的偏差-方差分解"><a href="#6-分类问题中的偏差-方差分解" class="headerlink" title="6. 分类问题中的偏差-方差分解"></a><strong>6. 分类问题中的偏差-方差分解</strong></h4><ul>
<li>与回归问题相比, 分类问题中的偏差-方差分解更加复杂。</li>
<li>目前尚无统一的分解方法, 不同的研究提出了不同的形式化方法。</li>
</ul>
<hr>
<h1 id="经验风险最小化（Empirical-Risk-Minimization-ERM）"><a href="#经验风险最小化（Empirical-Risk-Minimization-ERM）" class="headerlink" title="经验风险最小化（Empirical Risk Minimization, ERM）"></a>经验风险最小化（Empirical Risk Minimization, ERM）</h1><h2 id="经验风险最小化是机器学习中最基本的学习算法之一，其目标是通过最小化训练误差来拟合模型参数。"><a href="#经验风险最小化是机器学习中最基本的学习算法之一，其目标是通过最小化训练误差来拟合模型参数。" class="headerlink" title="经验风险最小化是机器学习中最基本的学习算法之一，其目标是通过最小化训练误差来拟合模型参数。"></a>经验风险最小化是机器学习中最基本的学习算法之一，其目标是通过最小化训练误差来拟合模型参数。</h2><h2 id="1-基本假设"><a href="#1-基本假设" class="headerlink" title="1. 基本假设"></a><strong>1. 基本假设</strong></h2><ul>
<li><strong>同分布假设</strong>：训练数据和测试数据来自相同的分布 ( D )，这是 PAC（Probably Approximately Correct）学习理论中的一个重要假设。</li>
<li><strong>独立同分布（i.i.d.）</strong>：训练样本是独立同分布的。</li>
</ul>
<hr>
<h2 id="2-线性分类问题"><a href="#2-线性分类问题" class="headerlink" title="2. 线性分类问题"></a><strong>2. 线性分类问题</strong></h2><ul>
<li>假设模型为线性分类器：<br>$$<br>h_\theta(x) &#x3D; \mathbb{1}{\theta^T x \geq 0}<br>$$<br>其中 ( \theta ) 是模型参数。</li>
<li><strong>参数拟合方法</strong>：通过最小化训练误差来找到最优参数：<br>$$<br>\hat{\theta} &#x3D; \arg\min_\theta \hat{\epsilon}(h_\theta)<br>$$<br>其中 ( \hat{\epsilon}(h_\theta) ) 是训练误差。</li>
</ul>
<hr>
<h2 id="3-经验风险最小化（ERM）"><a href="#3-经验风险最小化（ERM）" class="headerlink" title="3. 经验风险最小化（ERM）"></a><strong>3. 经验风险最小化（ERM）</strong></h2><ul>
<li><strong>定义</strong>：ERM 是通过最小化训练误差来学习模型参数的过程：<br>$$<br>\hat{h} &#x3D; h_{\hat{\theta}}<br>$$<br>其中 ( \hat{h} ) 是学习算法输出的假设。</li>
<li><strong>直观解释</strong>：ERM 是最基本的学习算法，目标是使模型在训练集上的表现最优。</li>
<li><strong>扩展</strong>：许多算法（如逻辑回归）可以看作是 ERM 的近似。</li>
</ul>
<hr>
<h2 id="4-假设类（Hypothesis-Class）"><a href="#4-假设类（Hypothesis-Class）" class="headerlink" title="4. 假设类（Hypothesis Class）"></a><strong>4. 假设类（Hypothesis Class）</strong></h2><p>这里可以看一下讲义的对应内容，讲义对应内容大概讲解了假设类和打散 shatter</p>
<ul>
<li><strong>定义</strong>：假设类 ( H ) 是学习算法考虑的所有分类器的集合。<ul>
<li>对于线性分类问题：<br>$$<br>H &#x3D; {h_\theta : h_\theta(x) &#x3D; \mathbb{1}{\theta^T x \geq 0}, \theta \in \mathbb{R}^{d+1}}<br>$$<br>即所有线性分类器的集合。</li>
<li>对于神经网络，( H ) 可以是某种网络结构表示的所有分类器的集合。</li>
</ul>
</li>
<li><strong>ERM 的抽象形式</strong>：ERM 可以看作是在假设类 ( H ) 上的最小化问题：<br>$$<br>\hat{h} &#x3D; \arg\min_{h \in H} \hat{\epsilon}(h)<br>$$</li>
</ul>
<h3 id="有限假设类下的学习理论"><a href="#有限假设类下的学习理论" class="headerlink" title="有限假设类下的学习理论"></a>有限假设类下的学习理论</h3><p>当假设类 ( H ) 是有限的时候，我们可以通过概率工具（如霍夫丁不等式和联合界）来推导泛化误差的界。以下是核心内容和推导过程的总结：</p>
<hr>
<h4 id="1-问题设定-1"><a href="#1-问题设定-1" class="headerlink" title="1. 问题设定"></a><strong>1. 问题设定</strong></h4><ul>
<li>假设类 ( H &#x3D; {h_1, h_2, \dots, h_k} ) 是有限的，包含 ( k ) 个假设。</li>
<li>每个假设 ( h_i ) 是从输入空间 ( X ) 到输出空间 ({0, 1}) 的映射。</li>
<li>经验风险最小化（ERM）选择训练误差最小的假设 ( \hat{h} )。</li>
</ul>
<hr>
<h4 id="2-训练误差与泛化误差的关系"><a href="#2-训练误差与泛化误差的关系" class="headerlink" title="2. 训练误差与泛化误差的关系"></a><strong>2. 训练误差与泛化误差的关系</strong></h4><ul>
<li>对于任意固定的假设 ( h_i )，定义随机变量 ( Z &#x3D; \mathbb{1}{h_i(x) \neq y} )，其中 ( (x, y) \sim D )。</li>
<li>训练误差 ( \hat{\epsilon}(h_i) ) 是 ( n ) 个独立同分布的 ( Z_j ) 的均值：<br>$$<br>\hat{\epsilon}(h_i) &#x3D; \frac{1}{n} \sum_{j&#x3D;1}^n Z_j<br>$$</li>
<li>泛化误差 ( \epsilon(h_i) ) 是 ( Z ) 的期望值：<br>$$<br>\epsilon(h_i) &#x3D; \mathbb{E}[Z]<br>$$</li>
</ul>
<hr>
<h4 id="3-霍夫丁不等式"><a href="#3-霍夫丁不等式" class="headerlink" title="3. 霍夫丁不等式"></a><strong>3. 霍夫丁不等式</strong></h4><ul>
<li>对于任意固定的 ( h_i )，霍夫丁不等式给出了训练误差与泛化误差之间偏差的概率上界：<br>$$<br>P(|\epsilon(h_i) - \hat{\epsilon}(h_i)| &gt; \gamma) \leq 2 \exp(-2\gamma^2 n)<br>$$</li>
</ul>
<hr>
<h4 id="4-联合界与一致收敛"><a href="#4-联合界与一致收敛" class="headerlink" title="4. 联合界与一致收敛"></a><strong>4. 联合界与一致收敛</strong></h4><ul>
<li>定义事件 ( A_i ) 为 ( |\epsilon(h_i) - \hat{\epsilon}(h_i)| &gt; \gamma )。</li>
<li>使用联合界，对所有 ( h \in H ) 的偏差概率进行约束：<br>$$<br>P(\exists h \in H: |\epsilon(h) - \hat{\epsilon}(h)| &gt; \gamma) \leq 2k \exp(-2\gamma^2 n)<br>$$</li>
<li>因此，一致收敛的概率为：<br>$$<br>P(\forall h \in H: |\epsilon(h) - \hat{\epsilon}(h)| \leq \gamma) \geq 1 - 2k \exp(-2\gamma^2 n)<br>$$</li>
</ul>
<hr>
<h4 id="5-样本复杂度"><a href="#5-样本复杂度" class="headerlink" title="5. 样本复杂度"></a><strong>5. 样本复杂度</strong></h4><ul>
<li>给定 ( \gamma ) 和 ( \delta &gt; 0 )，为了以至少 ( 1 - \delta ) 的概率保证一致收敛，所需的样本量 ( n ) 为：<br>$$<br>n \geq \frac{1}{2\gamma^2} \log \frac{2k}{\delta}<br>$$</li>
<li>样本复杂度 ( n ) 与假设类大小 ( k ) 呈对数关系，这是该理论的关键性质。</li>
</ul>
<hr>
<h4 id="6-泛化误差的界"><a href="#6-泛化误差的界" class="headerlink" title="6. 泛化误差的界"></a><strong>6. 泛化误差的界</strong></h4><ul>
<li>定义 ( h^* &#x3D; \arg\min_{h \in H} \epsilon(h) ) 为假设类 ( H ) 中最优假设。</li>
<li>如果一致收敛成立，则 ERM 选择的假设 ( \hat{h} ) 的泛化误差满足：<br>$$<br>\epsilon(\hat{h}) \leq \epsilon(h^*) + 2\gamma<br>$$</li>
<li>这表明，( \hat{h} ) 的泛化误差最多比最优假设 ( h^* ) 差 ( 2\gamma )。</li>
</ul>
<hr>
<h4 id="7-定理总结"><a href="#7-定理总结" class="headerlink" title="7. 定理总结"></a><strong>7. 定理总结</strong></h4><ul>
<li><strong>定理</strong>：对于有限假设类 ( H )（( |H| &#x3D; k )），给定 ( n ) 和 ( \delta )，以至少 ( 1 - \delta ) 的概率，有：<br>$$<br>\epsilon(\hat{h}) \leq \left(\min_{h \in H} \epsilon(h)\right) + 2\sqrt{\frac{1}{2n} \log \frac{2k}{\delta}}<br>$$</li>
<li><strong>推论</strong>：给定 ( \delta ) 和 ( \gamma )，为了满足 ( \epsilon(\hat{h}) \leq \min_{h \in H} \epsilon(h) + 2\gamma )，所需的样本量为：<br>$$<br>n &#x3D; O\left(\frac{1}{\gamma^2} \log \frac{k}{\delta}\right)<br>$$</li>
</ul>
<hr>
<h4 id="8-偏差-方差权衡"><a href="#8-偏差-方差权衡" class="headerlink" title="8. 偏差-方差权衡"></a><strong>8. 偏差-方差权衡</strong></h4><ul>
<li>当假设类 ( H ) 增大时：<ul>
<li><strong>偏差</strong>（第一项 ( \min_{h \in H} \epsilon(h) )）会减小，因为优化空间更大。</li>
<li><strong>方差</strong>（第二项 ( 2\sqrt{\frac{1}{2n} \log \frac{2k}{\delta}} )）会增大，因为假设类大小 ( k ) 增加。</li>
</ul>
</li>
<li>这体现了模型选择中的偏差-方差权衡。</li>
</ul>
<hr>
<h3 id="无限假设类"><a href="#无限假设类" class="headerlink" title="无限假设类"></a>无限假设类</h3><h4 id="VC维与泛化误差"><a href="#VC维与泛化误差" class="headerlink" title="VC维与泛化误差"></a>VC维与泛化误差</h4><p>在机器学习理论中，VC维（Vapnik-Chervonenkis dimension）是衡量假设类（hypothesis class）复杂度的重要工具。</p>
<h5 id="基于VC维的核心结论："><a href="#基于VC维的核心结论：" class="headerlink" title="基于VC维的核心结论："></a>基于VC维的核心结论：</h5><ol>
<li><p><strong>VC维的定义</strong>：</p>
<ul>
<li>为了证明假设类 ( H ) 的VC维至少为 ( D )，只需证明存在一个大小为 ( D ) 的数据集能够被 ( H ) 打散（shatter）。</li>
</ul>
</li>
<li><p><strong>Vapnik定理</strong>：</p>
<ul>
<li>假设 ( H ) 的VC维为 ( D )，则在概率至少为 ( 1 - \delta ) 的情况下，对于所有 ( h \in H )，泛化误差 ( \epsilon(h) ) 和训练误差 ( \hat{\epsilon}(h) ) 之间的差距满足：<br>[<br>|\epsilon(h) - \hat{\epsilon}(h)| \leq O\left(\sqrt{\frac{D}{n} \log \frac{n}{D} + \frac{1}{n} \log \frac{1}{\delta}}\right)<br>]</li>
<li>这意味着，随着样本数量 ( n ) 的增加，泛化误差会逐渐收敛。</li>
</ul>
</li>
<li><p><strong>推论</strong>：</p>
<ul>
<li>为了确保 ( |\epsilon(h) - \hat{\epsilon}(h)| \leq \gamma ) 对所有 ( h \in H ) 成立（即 ( \hat{\epsilon}(h) \leq \epsilon(h^*) + 2\gamma )），所需的训练样本数量 ( n ) 与VC维 ( D ) 成线性关系，即：<br>[<br>n &#x3D; O_{\gamma,\delta}(D)<br>]</li>
</ul>
</li>
<li><p><strong>样本复杂度与参数数量的关系</strong>：</p>
<ul>
<li>对于大多数假设类，VC维与参数数量大致成线性关系。<strong>因此，为了达到接近最优分类器的泛化误差，所需的训练样本数量通常与假设类的参数数量成线性关系</strong>。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="无偏性与偏差"><a href="#无偏性与偏差" class="headerlink" title="无偏性与偏差"></a><strong>无偏性与偏差</strong></h2><ol>
<li><p><strong>采样与目标分布</strong>：</p>
<ul>
<li>设采样数目为 ( M ), 数据集为 ( X ), 采样分布为 ( \hat{Y} ), 目标分布为 ( Y )。</li>
<li>当 ( M ) 趋近于正无穷时：<br>$$<br>\lim_{M\to\infty} D(\hat{Y}) &#x3D; D(Y)<br>$$<br>即采样分布 ( \hat{Y} ) 会收敛到目标分布 ( Y )。</li>
</ul>
</li>
<li><p><strong>无偏性</strong>：</p>
<ul>
<li>如果对于任意的 ( M ), 都有：<br>$$<br>E(\hat{Y}) &#x3D; E(Y)<br>$$<br>则学习算法是无偏的。</li>
<li>无偏性意味着学习算法的期望预测值等于真实值。</li>
</ul>
</li>
<li><p><strong>偏差（Bias）</strong>：</p>
<ul>
<li>偏差是模型预测值的期望与真实值之间的差异：<br>$$<br>\text{Bias} &#x3D; E(\hat{Y}) - Y<br>$$</li>
<li>高偏差通常意味着模型欠拟合（underfitting）, 无法捕捉数据的真实分布。</li>
</ul>
</li>
</ol>
<hr>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a><strong>正则化</strong></h1><p>正则化是一种用于减少模型过拟合（overfitting）的技术, 通过在损失函数中添加惩罚项来限制模型复杂度。常用的正则化方法包括 L1 正则化和 L2 正则化。使用正则化相当于缩小了模型的假设空间(hypothesis space) 这种过程可能会增加模型的<strong>bias</strong>。</p>
<h2 id="1-L1-正则化"><a href="#1-L1-正则化" class="headerlink" title="1. L1 正则化"></a><strong>1. L1 正则化</strong></h2><ol>
<li><p><strong>定义</strong>：</p>
<ul>
<li>L1 正则化在损失函数中添加模型参数的绝对值之和：<br>$$<br>J(\theta) &#x3D; \text{Loss}(\theta) + \lambda \sum_{i&#x3D;1}^{n} |\theta_i|<br>$$<br>其中：<ul>
<li>( \text{Loss}(\theta) ) 是原始损失函数。</li>
<li>( \lambda ) 是正则化系数, 控制正则化的强度。</li>
<li>( \theta_i ) 是模型参数。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>作用</strong>：</p>
<ul>
<li>L1 正则化会使得部分参数变为零, 从而实现特征选择（feature selection）。</li>
<li>适用于稀疏模型（sparse model）, 即只有少数特征对预测结果有显著影响的情况。</li>
</ul>
</li>
<li><p><strong>优点</strong>：</p>
<ul>
<li>可以生成稀疏解, 减少模型复杂度。</li>
<li>适用于高维数据, 能够自动选择重要特征。</li>
</ul>
</li>
<li><p><strong>缺点</strong>：</p>
<ul>
<li>在特征相关性较高时, 可能会随机选择其中一个特征, 而忽略其他相关特征。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="2-L2-正则化"><a href="#2-L2-正则化" class="headerlink" title="2. L2 正则化"></a><strong>2. L2 正则化</strong></h2><ol>
<li><p><strong>定义</strong>：</p>
<ul>
<li>L2 正则化在损失函数中添加模型参数的平方和：<br>$$<br>J(\theta) &#x3D; \text{Loss}(\theta) + \lambda \sum_{i&#x3D;1}^{n} \theta_i^2<br>$$<br>其中：<ul>
<li>( \text{Loss}(\theta) ) 是原始损失函数。</li>
<li>( \lambda ) 是正则化系数, 控制正则化的强度。</li>
<li>( \theta_i ) 是模型参数。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>作用</strong>：</p>
<ul>
<li>L2 正则化会使得参数值趋近于零, 但不会完全为零。</li>
<li>适用于需要平滑模型参数的情况。</li>
</ul>
</li>
<li><p><strong>优点</strong>：</p>
<ul>
<li>能够有效防止过拟合, 提高模型的泛化能力。</li>
<li>对特征相关性较高的数据表现较好。</li>
</ul>
</li>
<li><p><strong>缺点</strong>：</p>
<ul>
<li>无法生成稀疏解, 所有特征都会被保留。</li>
</ul>
</li>
</ol>
<hr>
<h2 id="L1-和-L2-正则化的比较"><a href="#L1-和-L2-正则化的比较" class="headerlink" title="L1 和 L2 正则化的比较"></a><strong>L1 和 L2 正则化的比较</strong></h2><table>
<thead>
<tr>
<th>特性</th>
<th>L1 正则化</th>
<th>L2 正则化</th>
</tr>
</thead>
<tbody><tr>
<td><strong>惩罚项</strong></td>
<td>( \lambda \sum_{i&#x3D;1}^{n}</td>
<td>\theta_i</td>
</tr>
<tr>
<td><strong>解的性质</strong></td>
<td>稀疏解（部分参数为零）</td>
<td>非稀疏解（参数趋近于零）</td>
</tr>
<tr>
<td><strong>特征选择</strong></td>
<td>支持</td>
<td>不支持</td>
</tr>
<tr>
<td><strong>对异常值的鲁棒性</strong></td>
<td>较强</td>
<td>较弱</td>
</tr>
<tr>
<td><strong>计算复杂度</strong></td>
<td>较高（不可导）</td>
<td>较低（可导）</td>
</tr>
</tbody></table>
<hr>
<h2 id="正则化的选择"><a href="#正则化的选择" class="headerlink" title="正则化的选择"></a><strong>正则化的选择</strong></h2><ol>
<li><p><strong>L1 正则化的适用场景</strong>：</p>
<ul>
<li>特征维度较高, 且只有少数特征对预测结果有显著影响。</li>
<li>需要进行特征选择。</li>
</ul>
</li>
<li><p><strong>L2 正则化的适用场景</strong>：</p>
<ul>
<li>特征维度较高, 且特征之间存在相关性。</li>
<li>需要平滑模型参数, 防止过拟合。</li>
</ul>
</li>
<li><p><strong>结合使用（Elastic Net）</strong>：</p>
<ul>
<li>在某些情况下, 可以同时使用 L1 和 L2 正则化, 称为 Elastic Net：<br>$$<br>J(\theta) &#x3D; \text{Loss}(\theta) + \lambda_1 \sum_{i&#x3D;1}^{n} |\theta_i| + \lambda_2 \sum_{i&#x3D;1}^{n} \theta_i^2<br>$$</li>
<li>Elastic Net 结合了 L1 和 L2 正则化的优点, 适用于高维数据且特征之间存在相关性的情况。</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/02/13/Lecture9/" data-id="cm7e2695o0004p0tx9iwbcskm" data-title="Lecture9" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/2018Autumn/" rel="tag">2018Autumn</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2025/02/21/%E4%BF%A1%E6%81%AF%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          信息与网络安全课程设计
        
      </div>
    </a>
  
  
    <a href="/2025/02/13/%E8%BD%AF%E4%BB%B6%E5%AE%89%E5%85%A8%E5%A4%8D%E4%B9%A0/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">软件安全复习</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/2018Autumn/" rel="tag">2018Autumn</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/2018Autumn/" style="font-size: 10px;">2018Autumn</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/02/21/%E4%BF%A1%E6%81%AF%E4%B8%8E%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1/">信息与网络安全课程设计</a>
          </li>
        
          <li>
            <a href="/2025/02/13/Lecture9/">Lecture9</a>
          </li>
        
          <li>
            <a href="/2025/02/13/%E8%BD%AF%E4%BB%B6%E5%AE%89%E5%85%A8%E5%A4%8D%E4%B9%A0/">软件安全复习</a>
          </li>
        
          <li>
            <a href="/2025/02/13/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>